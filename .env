#Enter your whisper model, see VRAM requirement for further details at whisper Github | tiny, base, small, tiny.en, base.en
WHISPER_MODEL = base

#Use the (roughly 4x faster) whisper-turbo library. Requires CUDA Toolkit and cuDNN downloaded to work with GPU.
FASTER_WHISPER = OFF
FASTER_WHISPER_CPU_TRANSCRIPTION = OFF

#Make the speech to text split into chunks and calculate as you speak? (Lower = less percise, but calculates more often. 300 - 1200 is resonable)
WHISPER_CHUNKY = ON
WHISPER_CHUNKY_RATE = 710

#Name that you want your bot/waifu to have (used in like 2 places, unimportant)
# CHAR_NAME = Waifu

#Use the name of the Ooobabooga character card you want.
# CHARACTER_CARD = Waifu

#Enter your name that you entered while creating Character card. Typically User, You or if have entered your name.
YOUR_NAME = You

#Decide if hotkeys should be on or off when the program/waifu first boots. Valid values are "ON" and "OFF"
HOTKEYS_BOOT = OFF

#Decide if "Newline Cut" and "RP Supression" should start "ON" or "OFF".
NEWLINE_CUT_BOOT = OFF
RP_SUP_BOOT = ON

#API type (either Oobabooga or Ollama. For other Openai-styled endpoints, do Oobabooga)
API_TYPE = Oobabooga

#Ollama Options. A visual model only runs when using multimodal / when sending images. If pointed at the same model, it can be used for both (saves VRAM).
#Visual model only loaded if MODULE_VISUAL is "ON".
ZW_OLLAMA_MODEL = "nsheth/llama-3-lumimaid-8b-v0.1-iq-imatrix"
ZW_OLLAMA_MODEL_VISUAL = "nsheth/llama-3-lumimaid-8b-v0.1-iq-imatrix"

#Use the streaming API for Oobabooga/Ollama? Disable this if you are having issues getting responses.
API_STREAM_CHATS = OFF

#Maximum context length to tell the model to look at. More takes more VRAM and longer to generate, generally.
#TOKEN_LIMIT is for the actual ML portion and the actual cutoff, MESSAGE_PAIR_LIMIT just decides how much to send to it.
TOKEN_LIMIT = 4096
MESSAGE_PAIR_LIMIT = 10

#Share the current time with the bot?
TIME_IN_ENCODING = ON

#Info for the model link
#Works with any OpenAI-compatible LLM server, such as oobabooga/text-generation-webui, ollama in server mode, or other OpenAI endpoints.
HOST_PORT = 127.0.0.1:5000
IMG_PORT = 127.0.0.1:5007
VISUAL_CHARACTER_NAME = Z-WAIF-VisualAssist
VISUAL_PRESET_NAME = Z-WAIF-VisualPreset

#How should we scale the image? 1.0 = Small (for Oobabooga), 1.6 = Big (for Ollama). Sane values are 0.4 - 4.0.
IMG_SCALE = 1.0

#What you want your VTuber's eyes to move to. Default gives control to the model. Values can be: "Faces", "Random", "None". Requires setting up in the model.
EYES_FOLLOW = "None"
EYES_START_ID = 14

#Decides of each of these modules should be running. Recommended to turn on RAG after a few hours of use, for better memory. Valid "ON" or "OFF".
MODULE_MINECRAFT = OFF
MODULE_GAMING = OFF
MODULE_ALARM = OFF
MODULE_VTUBE = OFF
MODULE_DISCORD = OFF
MODULE_RAG = ON
MODULE_VISUAL = OFF
